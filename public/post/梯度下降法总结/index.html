<!DOCTYPE html>
<html lang="en-us">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    
    <title>梯度下降法总结 - (*´･д･)?</title>
    <meta property="og:title" content="梯度下降法总结 - (*´･д･)?">
    

    
      
    

    

    
    

    <link rel="stylesheet" href="/css/style.css" />
    <link rel="stylesheet" href="/css/fonts.css" />
    <link rel="stylesheet" href="/css/custom.css" />

  </head>

  
  <body class="post">
    <header class="masthead">
      <h1><a href="/">(*´･д･)?</a></h1>



      <nav class="menu">
        <input id="menu-check" type="checkbox" />
        <label id="menu-label" for="menu-check" class="unselectable">
          <span class="icon close-icon">✕</span>
          <span class="icon open-icon">☰</span>
          <span class="text">Menu</span>
        </label>
        <ul>
        
        
        <li><a href="/">Rencent</a></li>
        
        <li><a href="/tags/">Tags</a></li>
        
        <li><a href="/skilltree/">SkillTree</a></li>
        
        <li><a href="/bookmarks/">BookMarks</a></li>
        
        <li><a href="/about/">About</a></li>
        
        <li><a href="/contact/">Contact</a></li>
        
        
        </ul>
      </nav>
    </header>

    <article class="main">
      <header class="title">
      
<h1>梯度下降法总结</h1>

<h3>
  2017-05-20</h3>
<hr>


      </header>





<h1 id="传统的按照每次迭代取的样本数来分">传统的按照每次迭代取的样本数来分</h1>

<h3 id="batch-gradient-descent-全量梯度下降">Batch Gradient Descent 全量梯度下降</h3>

<p>每次都取整个训练集的样本来更新。<br />
<strong>缺点</strong>:计算慢，不能在线更新模型。</p>

<h3 id="stochastic-gradient-descent-随机梯度下降">Stochastic gradient descent 随机梯度下降</h3>

<p>每次只取训练集中的1个样本进行训练。
<strong>优点</strong>：训练速度快，模型可在线更新，能够有几率跳出局部最小点。
<strong>缺点</strong>：单个样本训练出来的gradient波动比较大。</p>

<h3 id="mini-batch-gradient-descent-小批量梯度下降">Mini-batch gradient descent 小批量梯度下降</h3>

<p>每次取一小撮样本来更新参数。
<strong>优点</strong>：一定程度上克服了前两个的缺点，训练相对稳定且速度快。</p>

<p><strong>梯度下降最关键的问题在于学习率</strong>：<br />
一是学习率大了会导致不能收敛，在极值点附近震荡，学习率小了收敛速度慢。<br />
二是传统的学习率是定死的，每个特征的学习率都一样。<br />
为了解决这个学习率的问题，众位先贤相出了很多不同的办法来调节学习率，核心的目的就是：<strong>让学习率自动自适应调节</strong>。<br />
下面就来总结不同的梯度下降优化算法。这要分3大类：</p>

<ul>
<li><strong>基于梯度</strong></li>
<li><strong>基于动量</strong></li>
<li><strong>梯度和动量相结合</strong>

<br /></li>
</ul>

<h1 id="梯度下降优化算法">梯度下降优化算法</h1>

<h3 id="基于梯度">基于梯度</h3>

<h4 id="adagrad">Adagrad</h4>

<p><div align=center><img src="http://markdown0327.oss-cn-beijing.aliyuncs.com/18-3-20/83222978.jpg"/></div>
* 图片引自NTU李宏毅教授的slides</p>

<p>adagrad就是每一次更新参数学习率都会除上之前每一步的梯度的和的开方(不开根号性能变差)，在梯度大的方向，学习率会小一些，而在梯度小的方向，学习率就会相对较大。</p>

<h4 id="rmsprop">RMSprop</h4>

<p>Adagrad有两个问题：一是是有时候它的学习率衰减得太快了，二是在实际中某一个参数的实际梯度可能是有急有缓的，这样Adagrad就没有办法很好的处理这种情况。比如前面急坡时学习率就已经衰减到一个比较小的值了，但是急坡后可能是缓坡，此时我们需要一个比原来更大的学习率，然而Adagrad没有办法给我们。所以要寻求一个对近期的梯度变化也能做出响应的算法。比如说RMSprop。
<div align=center><img src="http://markdown0327.oss-cn-beijing.aliyuncs.com/18-3-20/66213274.jpg"/></div>
RMSprop每一次更新都会考虑之前的&amp;delta&amp;和这一步的梯度，参数&amp;alpha&amp;就是用来权衡是考虑当下多一些还是考虑之前的多一些。比如当&amp;alpha&amp;&lt;0.5时，开方中的第一项的权重就相对比第二项小，这时就是更多的着眼于当下的梯度。</p>

<h3 id="基于动量">基于动量</h3>

<h4 id="momentum">momentum</h4>

<p><div align=center><img src="http://markdown0327.oss-cn-beijing.aliyuncs.com/18-3-20/91409062.jpg.jpg"/></div>
基于动量的参数更新每次减掉的不是梯度，而是动量，而动量又是上一次动量和这一次梯度×学习率的和，因此是参数的更新具有了一定的惯性。<br />
<div align=center><img src="http://markdown0327.oss-cn-beijing.aliyuncs.com/18-3-20/5652729.jpg"/></div>
比如当小球卡在local minuma时，由于之前的动量的存在，及时在本次迭代梯度为0的情况下，小球能有机会向右移动，跳出局部最小区域。这就是动量的核心思想。</p>

<h3 id="梯度和动量结合">梯度和动量结合</h3>

<h4 id="adam">Adam</h4>

<p>Adam(Adaptive Moment Estimation)本质上是带有动量项的RMSprop，它利用梯度的一阶矩估计和二阶矩估计动态调整每个参数的学习率。Adam的优点主要在于经过偏置校正后，每一次迭代学习率都有个确定范围，使得参数比较平稳。
<div align=center><img src="http://markdown0327.oss-cn-beijing.aliyuncs.com/18-3-20/96941887.jpg"/></div>
特点：</p>

<ul>
<li>结合了Adagrad善于处理稀疏梯度和RMSprop善于处理非平稳目标的优点</li>
<li>对内存需求较小</li>
<li>为不同的参数计算不同的自适应学习率</li>
<li>也适用于大多非凸优化</li>
<li>适用于大数据集和高维空间</li>
</ul>


  <footer>
  
<nav class="post-nav">
  <span class="nav-prev">&larr; <a href="/post/leetcode_issomorphic/">leetcode_isSomorphic</a></span>
  <span class="nav-next"><a href="/post/impala&#43;spark%E6%93%8D%E4%BD%9C%E5%AE%9E%E8%B7%B5/">Impala&#43;Spark操作实践</a> &rarr;</span>
</nav>





<script src="//yihui.name/js/math-code.js"></script>
<script async src="//cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML"></script>

<script async src="//yihui.name/js/center-img.js"></script>

  

  
  
<script async src="//cdn.bootcss.com/video.js/6.2.8/alt/video.novtt.min.js"></script>
<script async src="//cdn.bootcss.com/mathjax/2.7.2/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  tex2jax: {inlineMath: [['$','$'],]}
});
</script>
  <hr>
  <div class="copyright">&copy; <a href="https://xrazor.org">xrazor</a> 2017 | <a href="https://github.com/razorrr">Github</a> | <a href="https://twitter.com/">Twitter</a></div>
  
  </footer>
  </article>
  
<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-113190132-2', 'auto');
ga('send', 'pageview');
</script>

  </body>
</html>

