<!DOCTYPE html>
<html lang="en-us">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    
    <title>Word2vec简单实践 - (*´･д･)?</title>
    <meta property="og:title" content="Word2vec简单实践 - (*´･д･)?">
    

    
      
    

    

    
    




    <link rel="stylesheet" href="/css/style.css" />
    <link rel="stylesheet" href="/css/fonts.css" />
    <link rel="stylesheet" href="/css/custom.css" />

  </head>

  
  <body class="post">
    <header class="masthead">
      <h1><a href="/">(*´･д･)?</a></h1>



      <nav class="menu">
        <input id="menu-check" type="checkbox" />
        <label id="menu-label" for="menu-check" class="unselectable">
          <span class="icon close-icon">✕</span>
          <span class="icon open-icon">☰</span>
          <span class="text">Menu</span>
        </label>
        <ul>
        
        
        <li><a href="/">Rencent</a></li>
        
        <li><a href="/tags/">Tags</a></li>
        
        <li><a href="/skilltree/">SkillTree</a></li>
        
        <li><a href="/bookmarks/">BookMarks</a></li>
        
        <li><a href="/about/">About</a></li>
        
        <li><a href="/contact/">Contact</a></li>
        
        
        </ul>
      </nav>
    </header>

    <article class="main">
      <header class="title">
      
<h1>Word2vec简单实践</h1>

<h3>
  2017-01-12</h3>
<hr>


      </header>





<h2 id="word2vec-简介">word2vec 简介</h2>

<p>word2vec是google的一个开源工具，能够根据输入的词的集合计算出词与词之间的距离。
它将term转换成向量形式，可以把对文本内容的处理简化为向量空间中的向量运算，计算出向量空间上的相似度，来表示文本语义上的相似度。
word2vec计算的是余弦值，距离范围为0-1之间，值越大代表两个词关联度越高。
词向量：用Distributed Representation表示词，通常也被称为“Word Representation”或“Word Embedding（嵌入）”。
安装Gensim 和分词工具</p>

<h2 id="gensim">Gensim</h2>

<p>这里主要通过Gensim模块来实现word2vec:</p>

<p><code>pip install gensim -i https://pypi.douban.com/simple</code>
后面用豆瓣的源会快一些。</p>

<h2 id="分词工具">分词工具</h2>

<p>分词工具有很多：中科院NLPIR，哈工大LTP，结巴分词等。我这里用的是结巴分词。如果没有也是先用pip安装。</p>

<pre><code>pip install jieba -i https://pypi.douban.com/simple
</code></pre>

<h2 id="语料准备">语料准备</h2>

<p>随便下载一部小说，我这里用的是&lt;天龙八部&gt;。下下来可能是乱码。可以自己写一段小代码重新加工一下:</p>

<pre><code>#!/usr/bin/env python
# -*- coding: utf-8 -*-
file_in = open('/home/luke/Downloads/tlbb.txt', 'r')
file_out = open('/home/luke/Downloads/tlbb_utf8.txt', 'w')
line = file_in.readline()
while line:
    newline = line.decode('GB18030').encode('utf-8')  #用GBK、GB2312都会出错
    print newline,
    print &gt;&gt; file_out, newline,
    line = file_in.readline()
file_in.close()
file_out.close()
</code></pre>

<h2 id="训练模型">训练模型</h2>

<pre><code># -*- coding: utf-8 -*-
import gensim.models.word2vec as w2v
model_file_name = 'tlbb_model'
sentences = w2v.LineSentence('/home/luke/Downloads/tlbb_segments.txt')
model = w2v.Word2Vec(sentences, size=20, window=5, min_count=5, workers=4)
model.save(model_file_name)
similar_list = model.most_similar(&quot;慕容复&quot;.decode('utf-8'), topn=10)
print model.similarity(&quot;慕容复&quot;.decode('utf-8'), &quot;段誉&quot;.decode('utf-8'))
for word, similar in similar_list:
    print word, similar
</code></pre>

<p>好了，大功告成，现在我们来试试跑一跑，恩，结果如下：</p>

<pre><code>0.922225864416
萧峰 0.940936326981
游坦之 0.924276173115
段誉 0.922225773335
乔峰 0.920364379883
丁春秋 0.91599714756
童姥 0.914369821548
段延庆 0.912800788879
鸠摩智 0.912337422371
段正淳 0.911094427109
邓百川 0.905085086823
</code></pre>

<p>慕容复和段誉的相关性是0.922，而与慕容相关性最高的是萧峰，果然是北萧峰南慕容，怎样都会被拿来相提并论。可以看出，排名前十的几个人也都是小说中与慕容复纠葛较多的人，也就是说我们的模型是有说服力的。
关于word2vec，还有许多有趣的应用，以后有时间再慢慢研究。</p>


  <footer>
  
<nav class="post-nav">
  <span class="nav-prev">&larr; <a href="/post/%E4%B8%BA%E4%BB%80%E4%B9%88%E8%A6%81%E6%8C%81%E7%BB%AD%E8%BE%93%E5%87%BA%E8%87%AA%E5%B7%B1%E7%9A%84%E7%9F%A5%E8%AF%86/">为什么要持续输出自己的知识</a></span>
  <span class="nav-next"><a href="/post/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E5%87%A0%E4%B8%AA%E8%A7%84%E5%AE%9A%E5%8A%A8%E4%BD%9C/">机器学习的几个规定动作</a> &rarr;</span>
</nav>





<script src="//yihui.name/js/math-code.js"></script>
<script async src="//cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML"></script>

<script async src="//yihui.name/js/center-img.js"></script>

  



<script src="//cdn.bootcss.com/highlight.js//highlight.min.js"></script>



<script>hljs.configure({languages: []}); hljs.initHighlightingOnLoad();</script>



  
  
<script async src="//cdn.bootcss.com/video.js/6.2.8/alt/video.novtt.min.js"></script>
<script async src="//cdn.bootcss.com/mathjax/2.7.2/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  tex2jax: {inlineMath: [['$','$'],]}
});
</script>
  <hr>
  <div class="copyright">&copy; <a href="https://xrazor.org">xrazor</a> 2017 | <a href="https://github.com/razorrr">Github</a> | <a href="https://twitter.com/">Twitter</a></div>
  
  </footer>
  </article>
  
<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-107683077-1', 'auto');
ga('send', 'pageview');
</script>

  </body>
</html>

