<!DOCTYPE html>
<html lang="en-us">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    
    <title>如何评价模型的好坏 - (*´･д･)?</title>
    <meta property="og:title" content="如何评价模型的好坏 - (*´･д･)?">
    

    
      
    

    

    
    

    <link rel="stylesheet" href="/css/style.css" />
    <link rel="stylesheet" href="/css/fonts.css" />
    <link rel="stylesheet" href="/css/custom.css" />

  </head>

  
  <body class="post">
    <header class="masthead">
      <h1><a href="/">(*´･д･)?</a></h1>



      <nav class="menu">
        <input id="menu-check" type="checkbox" />
        <label id="menu-label" for="menu-check" class="unselectable">
          <span class="icon close-icon">✕</span>
          <span class="icon open-icon">☰</span>
          <span class="text">Menu</span>
        </label>
        <ul>
        
        
        <li><a href="/">Rencent</a></li>
        
        <li><a href="/tags/">Tags</a></li>
        
        <li><a href="/skilltree/">SkillTree</a></li>
        
        <li><a href="/bookmarks/">BookMarks</a></li>
        
        <li><a href="/about/">About</a></li>
        
        <li><a href="/contact/">Contact</a></li>
        
        
        </ul>
      </nav>
    </header>

    <article class="main">
      <header class="title">
      
<h1>如何评价模型的好坏</h1>

<h3>
  2017-07-23</h3>
<hr>


      </header>





<p>这个问题可以分解为3个子问题：首先，评估模型时具体评估的是什么？其次，评估方法和过程是怎样的？最后，评估指标该如何选择？
下面将针对这三个子问题分别回答。</p>

<h3 id="1-评估模型时具体评估的是什么">1.评估模型时具体评估的是什么？</h3>

<p>总的来说评估的是模型的<strong>泛化能力</strong>。泛化能力通过泛化误差来体现，模型的泛化误差小，说明面对未知数据，模型也能根据学到的东西做出很好的预测，即泛化能力强。而<strong>泛化误差=偏差(Bias)+方差(Variance)+噪音(Noise)</strong>。偏差度量了算法的期望预测与真实结果的偏离程度，刻画了模型本身的拟合能力；方差度量了同样大小的训练集导致的学习性能的变化，刻画了数据扰动造成的影响。最后噪音表达了在当前任务上任何算法能达到的期望泛化误差的下界，即问题本身的难度。</p>

<h4 id="偏差过大是模型设计问题">偏差过大是模型设计问题</h4>

<p>如果在训练集上偏差很大，说明模型欠拟合，模型本身学习能力不强；如果在训练集上偏差小而在测试集上偏差很大，说明模型过拟合，学到了训练集中一些普适性不强的东西。</p>

<h4 id="方差过大是训练过程控制问题">方差过大是训练过程控制问题</h4>

<p>如果方差过大，说明训练出来的模型会随着数据集变动而变动。假设下式：
<center><strong>gap = validation error − training error</strong></center>
在衡量的时候，可通过改变训练集的大小，当训练集由小变大时，对于方差小的模型，gap会逐步比较小，反之则大。<br />
<strong>至此可以作如下总结</strong>：
高偏差低方差: 模型欠拟合，模型学习能力不足，需重新设计；低偏差高方差，说明模型存在过拟合的风险，需要增加数据或做正则化。</p>

<h3 id="2-模型评估的方法和过程是怎样的">2.模型评估的方法和过程是怎样的？</h3>

<p><strong>通过交叉验证法(CrossValidation)来做</strong>。交叉验证法将数据集分成n个大小相似的子集，同时子集的正负例分布需保持基本一致，然后用其中的n-1个子集做训练集，剩下的做测试集，重复n次训练，如此得到n次的结果（指标）的均值。</p>

<h3 id="3-评估指标该如何选择">3.评估指标该如何选择？</h3>

<p>常用的评估指标有以下几个：</p>

<h4 id="准确率">准确率</h4>

<p>对于二分类来说，如果样本数为n，正确预测的样本数为m，则准确率p=n/m，这是最基本的度量标准之一，准确率越高模型越好。但是实际任务中可能更关注的是“筛出来的正例有多少是真正例”或“真正例有多少被筛出来了”，这样的情况下准确率就不足以评估好坏了。</p>

<h4 id="查准率-precision-查全率-recall-f1和pr曲线">查准率(precision)、查全率(recall)、F1和PR曲线</h4>

<p>对于二分类来说预测结果可能有真正例（TP）、假正例（FP）、真负例（TN）、假负例（FN）。查准率P=TP/(TP+FP)，查全率R=TP/(TP+FN)。顾名思义，查准率高则说明查出来的大多是真的，查全率高说明真的大多被查出来了。这两个指标通常是越高越好，但是查准率和查全率是跷跷板的两头，此高彼低，因此有时候不太好衡量，所以又引入了F1指标，F1=1 /((1/P+1/R)*0.5), 这是P和R的调和平均值，另外，在某些场景下，对P、R的偏好不同，可对P和R做加权调和平均。对查准率和查全率。F1越高模型越好。另外，以P为纵轴R为横轴，可做PR曲线，直观的判断模型的表现，当一条PR曲线被另一条包住，则后者表现更好。</p>

<h4 id="roc和auc">ROC和AUC</h4>

<p>有些模型的预测结果是一个概率或数值，根据这个预测值与设定好的分类阈值比较后再确定分类。因此可以对样本预测结果排序后再根据需要选择不同的阈值。这是排序的质量就反应了模型泛化能力的好坏。ROC就是从这个思路切入的，它纵轴为真正例率(TPR=TP/ (TP+ FN))，横轴为假正例率(FPR=FP / (FP + TN))，排序后逐个把样本作为正例进行预测，每次计算出TPR和FPR，这样就可以做出ROC曲线。ROC曲线越靠近左上角模型越好，但是有时候不同的ROC曲线会相交，不太好判断孰优孰劣，因此又引入了AUC，是ROC与x轴围城的面积，AUC越高越好。<br />
综上所述，模型表现的好坏要结合具体的数据集和业务问题来综合考虑，有针对性地选择评估指标。</p>


  <footer>
  
<nav class="post-nav">
  <span class="nav-prev">&larr; <a href="/post/%E5%AE%9E%E5%81%9Acnn%E6%89%8B%E5%86%99%E6%95%B0%E5%AD%97%E8%BE%A8%E8%AF%86/">实做CNN手写数字辨识</a></span>
  <span class="nav-next"><a href="/post/hugo&#43;nginx&#43;webhook%E5%9C%A8%E6%9C%8D%E5%8A%A1%E5%99%A8%E4%B8%8A%E6%90%AD%E5%BB%BA%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2%E5%B9%B6%E5%AE%9E%E7%8E%B0%E8%87%AA%E5%8A%A8%E5%8F%91%E5%B8%83/">Hugo&#43;nginx&#43;webhook:在服务器上搭建个人博客并实现自动发布</a> &rarr;</span>
</nav>





<script src="//yihui.name/js/math-code.js"></script>
<script async src="//cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML"></script>

<script async src="//yihui.name/js/center-img.js"></script>

  

  
  
<script async src="//cdn.bootcss.com/video.js/6.2.8/alt/video.novtt.min.js"></script>
<script async src="//cdn.bootcss.com/mathjax/2.7.2/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  tex2jax: {inlineMath: [['$','$'],]}
});
</script>
  <hr>
  <div class="copyright">&copy; <a href="https://xrazor.org">xrazor</a> 2017 | <a href="https://github.com/razorrr">Github</a> | <a href="https://twitter.com/">Twitter</a></div>
  
  </footer>
  </article>
  
<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-113190132-2', 'auto');
ga('send', 'pageview');
</script>

  </body>
</html>

