<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>机器学习 on (*´･д･)?</title>
    <link>http://xrazor.org/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/</link>
    <description>Recent content in 机器学习 on (*´･д･)?</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Sun, 25 Jun 2017 23:04:01 +0800</lastBuildDate>
    
	<atom:link href="http://xrazor.org/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>实做CNN手写数字辨识</title>
      <link>http://xrazor.org/post/%E5%AE%9E%E5%81%9Acnn%E6%89%8B%E5%86%99%E6%95%B0%E5%AD%97%E8%BE%A8%E8%AF%86/</link>
      <pubDate>Sun, 25 Jun 2017 23:04:01 +0800</pubDate>
      
      <guid>http://xrazor.org/post/%E5%AE%9E%E5%81%9Acnn%E6%89%8B%E5%86%99%E6%95%B0%E5%AD%97%E8%BE%A8%E8%AF%86/</guid>
      <description>开始   语言：Python2 框架：Keras Backend: tensorflow OS: Archlinux Package: numpy,pandas,seaborn,sklearn,etc Method: CPU(i7) 数据集: MNIST  模型结构：
In -&amp;gt; [[Conv2D-&amp;gt;relu]*2 -&amp;gt; MaxPool2D -&amp;gt; Dropout]*2 -&amp;gt; Flatten -&amp;gt; Dense -&amp;gt; Dropout -&amp;gt; Out.  现在让我们在Ipython里面跑。
数据预处理 import各种包 import pandas as pd import numpy as np import matplotlib.pyplot as plt import matplotlib.image as mpimg import seaborn as sns np.random.seed(2) # seed中使用数字相同，则每次程序中生成的随机数都相同 from sklearn.model_selection import train_test_split from sklearn.metrics import confusion_matrix import itertools from keras.</description>
    </item>
    
    <item>
      <title>机器学习的几个规定动作</title>
      <link>http://xrazor.org/post/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E5%87%A0%E4%B8%AA%E8%A7%84%E5%AE%9A%E5%8A%A8%E4%BD%9C/</link>
      <pubDate>Mon, 20 Mar 2017 00:05:02 +0800</pubDate>
      
      <guid>http://xrazor.org/post/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E5%87%A0%E4%B8%AA%E8%A7%84%E5%AE%9A%E5%8A%A8%E4%BD%9C/</guid>
      <description> 我们拿到一个机器学习项目之后，通常都会经过以下几个步骤：
1. Insight 数据洞察 统计机器学习一般是从已知数据集中训练模型，并用模型对未知数据进行预测。因此对已知数据的理解变得尤为重要。
Common Sense 善于利用常识 这个放在第一位，是因为刚开始做机器学习的同学，可能会觉得任何事情都从数据统计的角度去观察会很fancy。但实事是，这样的思维有时候会让我们只见树木不见森林，陷入到数据的细节里面。为了避免这种情况的发生，我们要做的就是首先通过肉眼观察数据，通过直觉来判断那几个Category有可能会在后面作为Features。 以Kaggle的入门Case——Titanic为例。Data Category不多:ID, 生还情况， 船舱等级， 名字， 性别， 年龄， 家属人数， 船票信息， 票价， 仓位， 登船码头。从直觉来看，与生还情况相关性最大的，应该是性别和年龄，因为在国外Children and Lady first的道德环境，小孩和女人的生还概率可能较大；此外，从阶级等级来看，有钱人可能会得到优先安排而有较高的生还概率，而有钱人一般住高级舱，Fare也比较高， 而年少多金的人（船舱等级 X 年龄），身份可能更为显赫。另外同船家属人数的多少，可能也是一个Feature，因为一家人可能会相互帮助从而提高生还率。等等。这样我们通过常识和直觉就能得到了几个方向的猜测，至于对与不对，可以留到后面再来验证。
Data Visualization 数据可视化 Python下用pandas很容易做数据统计描述，但是有时候数据不如图表那么直观，我们需要用matplotlib和seaborn等可视化工具来生成图表，帮助我们快速分析，并且在整个项目过程中，可视化是很重要的可能会被反复使用的技能，它能帮助我们保持思路清晰。 在这个过程中，上一步的许多猜测会得到进一步的验证。一边做可视化，一边确定哪些是要带入模型的Feature， 同时可能还要人工设计几个Features，比如Titanic case里面的（船舱等级X年龄）。
2. Preprocessing 数据预处理 缺失值填充
对于年龄等数值型的数据，可以通过Randomforest拟合出一个数据，但是由于年龄这种东西，并不是船舱等级、船票等其他类的数据一定存在太大关系，因此用这样的方法，虽然看起来fancy，但是并不一定有太大作用，可能直觉乱填一个效果都比之要好，因此也不能时刻迷信算法的结果。其他的思路还有取区间平均值，或者中位数。这都需要大量的试才能得到好的结果。并不是一蹴而就的事情。
0-1化 有些数据类别本身不是数值型的，而是一些字符型的类别，为了便于带入模型计算，我们要把它转化为0-1数据，也就是Dummy Code。这一步可以直接用pandas里面的get_dummies方法，实现很方便。
归一化 当数据与数据之间的范围相差很大，比如说年龄的范围从1岁到80岁，而船舱的等级就是1-3级，这样会导致带入模型使用梯度下降法或者拟牛顿法求解时，数据收敛很慢，甚至是不收敛。因此要对数据做scaling，就是归一化， 把数据归一到一个很小的范围，比如（-1，1）.可以调用sklearn的preprocessing处理。
数据弃用 代入模型的时候肯定不是所有数据都代入，选择好Features之后把不用的数据类别都drop掉。 通过以上的步骤我们得到了一组clean data，下一步就是构建模型了。但是不要看前面这两步简单，其实有很多判断和取舍，并且需要反复的测试对比，才能得到最终的Features，所以俗话说得好：数据和特征决定了机器学习的上限，而模型和算法只是不断的逼近这个上限。很多项目的大部分时间（&amp;gt;70%）实际上都花在特征工程上，可见其重要性。
3.Modeling 建模 选择方法 在做kaggle的时候，大家可以很方便的调用sklearn的各种机器学习方法，比如LR,SVM,NB,DT,KNN等等，反正就几行代码的事。事实上，我猜测，在工业界，实际选择方法的时候一是考虑模型的泛化能力，二是考虑模型的鲁棒性， 三是考虑模型的计算消耗， 四是考虑模型的解释性。因此并不像我们“游戏”里面选择那么自由。 在kaggle中我们可以根据项目要解决的问题，选择相对匹配的算法，比如项目要解决的是二分类问题，那么可能LR,SVM,DT是相对好的选择；如果项目是图像或语音识别，那可能CNN等深度学习算法优势会更大。等等。
模型评估  Optimization 性能优化
 基于数据的优化
 基于算法的优化
 基于参数的优化
 基于模型融合的优化
  </description>
    </item>
    
    <item>
      <title>10分钟训练一个神经网络</title>
      <link>http://xrazor.org/post/10%E5%88%86%E9%92%9F%E8%AE%AD%E7%BB%83%E4%B8%80%E4%B8%AA%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/</link>
      <pubDate>Sat, 18 Feb 2017 21:17:20 +0800</pubDate>
      
      <guid>http://xrazor.org/post/10%E5%88%86%E9%92%9F%E8%AE%AD%E7%BB%83%E4%B8%80%E4%B8%AA%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/</guid>
      <description>准备  OS:Archlinux 语言:Python2 框架:Keras(非GPU) IDE:Ipython 数据集:MNIST  开始 默认以上环境已经齐备，下面直接开始。
Step 1:把包导进来 import matplotlib.pyplot as plt # 1 from keras.datasets import mnist # 2 from keras.models import Sequential # 3 from keras.layers.core import Dense, Dropout, Activation # 4 from keras.utils import np_utils # 5  对应注释上的编号:
 画图的 内置数据集 Sequential的神经网络是线性的从头到尾不分叉的 Dense即fully connection, Dropout层防止过拟合，Actiction 即激活层 numpy移植版  Step 2:把数据load进来 nb_classes = 10 (X_train, y_train), (X_test, y_test) = mnist.load_data() # 1 print(&amp;quot;X_train original shape&amp;quot;, X_train.</description>
    </item>
    
    <item>
      <title>Word2vec简单实践</title>
      <link>http://xrazor.org/post/word2vec%E7%AE%80%E5%8D%95%E5%AE%9E%E8%B7%B5/</link>
      <pubDate>Thu, 12 Jan 2017 23:57:26 +0800</pubDate>
      
      <guid>http://xrazor.org/post/word2vec%E7%AE%80%E5%8D%95%E5%AE%9E%E8%B7%B5/</guid>
      <description>word2vec 简介 word2vec是google的一个开源工具，能够根据输入的词的集合计算出词与词之间的距离。 它将term转换成向量形式，可以把对文本内容的处理简化为向量空间中的向量运算，计算出向量空间上的相似度，来表示文本语义上的相似度。 word2vec计算的是余弦值，距离范围为0-1之间，值越大代表两个词关联度越高。 词向量：用Distributed Representation表示词，通常也被称为“Word Representation”或“Word Embedding（嵌入）”。 安装Gensim 和分词工具
Gensim 这里主要通过Gensim模块来实现word2vec:
pip install gensim -i https://pypi.douban.com/simple  后面用豆瓣的源会快一些。
分词工具 分词工具有很多：中科院NLPIR，哈工大LTP，结巴分词等。我这里用的是结巴分词。如果没有也是先用pip安装。
pip install jieba -i https://pypi.douban.com/simple  语料准备 随便下载一部小说，我这里用的是&amp;lt;天龙八部&amp;gt;。下下来可能是乱码。可以自己写一段小代码重新加工一下:
#!/usr/bin/env python # -*- coding: utf-8 -*- file_in = open(&#39;/home/luke/Downloads/tlbb.txt&#39;, &#39;r&#39;) file_out = open(&#39;/home/luke/Downloads/tlbb_utf8.txt&#39;, &#39;w&#39;) line = file_in.readline() while line: newline = line.decode(&#39;GB18030&#39;).encode(&#39;utf-8&#39;) #用GBK、GB2312都会出错 print newline, print &amp;gt;&amp;gt; file_out, newline, line = file_in.readline() file_in.close() file_out.close()  训练模型 # -*- coding: utf-8 -*- import gensim.</description>
    </item>
    
    <item>
      <title>信息熵、信息增益和Gini指数</title>
      <link>http://xrazor.org/post/%E4%BF%A1%E6%81%AF%E7%86%B5%E4%BF%A1%E6%81%AF%E5%A2%9E%E7%9B%8A%E5%92%8Cgini%E6%8C%87%E6%95%B0/</link>
      <pubDate>Mon, 21 Nov 2016 23:21:28 +0800</pubDate>
      
      <guid>http://xrazor.org/post/%E4%BF%A1%E6%81%AF%E7%86%B5%E4%BF%A1%E6%81%AF%E5%A2%9E%E7%9B%8A%E5%92%8Cgini%E6%8C%87%E6%95%B0/</guid>
      <description>在机器学习决策树中，引入里熵、信息增益、信息增益比和Gini指数，作为特征选择的准则。不同的算法对应了不同的特征选择准则，如ID3选用信息增益的大小作为特征选择判定的标准，C4.5则选用信息增益比为选择标准，CART则用Gini指数作为选择标准。那么，所引入的这几个指标的内涵意义是什么呢？本文将作一个简单的梳理。
信息熵 在信息论里面，熵被用来衡量一个随机变量出现的期望值，对不确定性的测量，是描述一个系统所需的最低存储单元。在信息世界，熵越高，所需传递的信息越丰富，熵越低，则意味着所需传输的信息越少。而在热力学里，熵越高，表示混乱度越高。 这三个解释乍一看毫无关联，实际上确实可以相互印证的。以抛硬币为例，自然条件下结果为正面或反面的概率均为50%， 此时结果的不确定性最高， 越混乱，而我们要描述这个结果所需的文字（暂且用文字的个数代表存储单元）越多，比如你会这么描述“结果可能是正面，也可能是反面”（加逗号14个字）；而当我们假定正面出现的概率为100%时，信息熵为0，不混乱很清晰，而此时我们描述这个结果所需的存储单元是很少的，比如说“是正面”（3个字）就能描述清楚。
信息增益 有了上面的概念，我们就可以结合ID3算法来看看信息增益是什么东西。 在ID3中，使用信息增益作为特征选择的准则，用H（D）表示数据集的经验熵，用H（D|A)表示在A特征下数据集D的经验条件熵，则信息增益g（D，A）= H(D) - H(D|A), 表示 有A 和 没A ，整个系统的不确定性变化了多少。这里要牢记， 熵高，是说明不确定很大，如果H（D）很大，而 H（D|A)很小，则g（D,A）会趋大， 说明特征A 的存在，大大降低了系统的不确定性， 因此它是一个关键的特征。
Gini指数 Gini指数是CART算法中生成分类树时用来选择特征的一个指标。</description>
    </item>
    
  </channel>
</rss>